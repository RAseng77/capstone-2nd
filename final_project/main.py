"""
Tupick â€” ì¡°ê°íˆ¬ì ì„œë¹„ìŠ¤ AI (RAG MVP)
-------------------------------------------------
Streamlit ì•± í•˜ë‚˜ë¡œ ìˆ˜ì§‘ â†’ ì¸ë±ì‹± â†’ ê²€ìƒ‰ â†’ ë¦¬í¬íŠ¸ ìƒì„±ê¹Œì§€ ìˆ˜í–‰í•©ë‹ˆë‹¤.

# âœ… ê¸°ëŠ¥
- URL ë‹¤ê±´ ì…ë ¥ í›„ ë³¸ë¬¸ í¬ë¡¤ë§(robots.txtëŠ” ë³„ë„ í™•ì¸ í•„ìš”)
- ë¬¸ë‹¨ ì²­í¬ + ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆë¡œ ì •ì œ
- bge-m3 ì„ë² ë”© + FAISS ë²¡í„° ì¸ë±ìŠ¤ ë¡œì»¬ ì €ì¥/ë¡œë“œ
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(BM25 ìœ ì‚¬ í‚¤ì›Œë“œ + ë²¡í„° ê²€ìƒ‰) ê°„ë‹¨ ë²„ì „
- Ollama(ë¡œì»¬ LLM) ë˜ëŠ” OpenAI API ì¤‘ ì‚¬ìš© ê°€ëŠ¥
- ê·¼ê±° ê°ì£¼ + ì•ˆì „ ë¬¸êµ¬ í¬í•¨ ë¦¬í¬íŠ¸ ìƒì„±

# ğŸ“¦ ìš”êµ¬ ë¼ì´ë¸ŒëŸ¬ë¦¬(ì˜ˆì‹œ)
pip install streamlit requests beautifulsoup4 sentence-transformers faiss-cpu rank-bm25 pydantic python-dotenv

# ğŸ”§ ì‚¬ì „ ì¤€ë¹„
1) ëª¨ë¸
   - (ê¶Œì¥) Ollama ì„¤ì¹˜ í›„: `ollama pull llama3.1:8b-instruct` ë˜ëŠ” Qwen2.5-7B-Instruct
   - OpenAIë¥¼ ì“¸ ê²½ìš° .envì— OPENAI_API_KEY ì„¸íŒ…
2) ì‹¤í–‰: `streamlit run app.py`

# âš ï¸ ì£¼ì˜
- ê° ì‚¬ì´íŠ¸ì˜ ì•½ê´€/robots.txtì™€ ì €ì‘ê¶Œì„ ì¤€ìˆ˜í•˜ì„¸ìš”.
- ë³¸ ì•±ì€ ì •ë³´ ì œê³µìš© ë°ëª¨ì…ë‹ˆë‹¤. íˆ¬ì ê¶Œìœ ê°€ ì•„ë‹™ë‹ˆë‹¤.
"""

import os
import re
import json
import time
import uuid
import math
import faiss
import queue
import base64
import hashlib
import requests
import streamlit as st

from io import BytesIO
from bs4 import BeautifulSoup
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Tuple
from urllib.parse import urlparse
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
from pydantic import BaseModel
from dotenv import load_dotenv

# ----------------------------
# Config
# ----------------------------
load_dotenv()
def _default_data_dir():
    # Use ASCII-only path on Windows to avoid Faiss unicode bug
    env = os.getenv("TUPICK_DATA_DIR")
    if env:
        return env
    if os.name == "nt":
        return r"C:\tupick_data"  # safe ASCII path
    return os.path.join(os.getcwd(), "tupick_data")

DATA_DIR = _default_data_dir()
INDEX_DIR = os.path.join(DATA_DIR, "index")
DOCS_PATH = os.path.join(DATA_DIR, "docs.jsonl" )
EMB_MODEL_NAME = os.getenv("EMB_MODEL_NAME", "BAAI/bge-m3")
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3.1:8b-instruct")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
TOP_K = int(os.getenv("TOP_K", 5))
CHUNK_SIZE = 550
CHUNK_OVERLAP = 60

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(INDEX_DIR, exist_ok=True)

# ----------------------------
# Data Schemas
# ----------------------------
class DocChunk(BaseModel):
    id: str
    source: str
    category: str
    title: str
    section: str
    url: str
    as_of_date: str
    text: str
    language: str = "ko-KR"

# ----------------------------
# Utils
# ----------------------------

def normalize_whitespace(s: str) -> str:
    return re.sub(r"\s+", " ", s).strip()


def chunk_text(text: str, size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:
    text = normalize_whitespace(text)
    tokens = text.split(" ")
    chunks = []
    i = 0
    while i < len(tokens):
        chunk = tokens[i : i + size]
        chunks.append(" ".join(chunk))
        i += max(1, size - overlap)
    return [c for c in chunks if len(c) > 20]


def fetch_url(url: str, timeout: int = 20, use_js: bool = False) -> Tuple[str, str]:
    """Return (title, main_text).
    1) Try requests+BS4
    2) (optional) Fallback to Playwright JS render for SPA/blocked pages
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36",
        "Accept-Language": "ko,ko-KR;q=0.9,en-US;q=0.8,en;q=0.7",
        "Cache-Control": "no-cache",
    }
    try:
        resp = requests.get(url, headers=headers, timeout=timeout)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")
        title = soup.title.text.strip() if soup.title else url
        for tag in soup(["script", "style", "noscript"]):
            tag.extract()
        body = normalize_whitespace(soup.get_text(" "))
        # If content too small, try JS rendering if allowed
        if use_js and len(body) < 800:
            raise RuntimeError("Body too small; try JS render")
        return title, body
    except Exception as e:
        if not use_js:
            raise
        # Playwright fallback
        try:
            from playwright.sync_api import sync_playwright
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                ctx = browser.new_context(locale="ko-KR", user_agent=headers["User-Agent"])
                page = ctx.new_page()
                page.goto(url, wait_until="networkidle", timeout=timeout*1000)
                title = page.title() or url
                body = page.locator("body").inner_text(timeout=timeout*1000)
                browser.close()
                body = normalize_whitespace(body)
                return title, body
        except Exception as e2:
            raise RuntimeError(f"fetch_url failed for {url}: {e2}")


def guess_category_from_url(url: str) -> str:
    host = urlparse(url).netloc.lower()
    if "tessa" in host:
        return "ë¯¸ìˆ í’ˆ/ëª…í’ˆ"
    if "art" in host:
        return "ë¯¸ìˆ í’ˆ"
    if "music" in host or "mucis" in host or "mucic" in host:
        return "ì €ì‘ê¶Œ/ìŒì›"
    if "kasa" in host or "lucent" in host:
        return "ë¶€ë™ì‚°"
    return "ì¡°ê°íˆ¬ì/ì¼ë°˜"


def now_date() -> str:
    import datetime as dt
    return dt.datetime.now().strftime("%Y-%m-%d")


def save_jsonl(items: List[Dict[str, Any]], path: str):
    with open(path, "a", encoding="utf-8") as f:
        for it in items:
            f.write(json.dumps(it, ensure_ascii=False) + "\n")


def load_jsonl(path: str) -> List[Dict[str, Any]]:
    if not os.path.exists(path):
        return []
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            rows.append(json.loads(line))
    return rows

# ----------------------------
# Embeddings & Index
# ----------------------------
@st.cache_resource(show_spinner=False)
def get_embedder():
    model = SentenceTransformer(EMB_MODEL_NAME)
    return model


def build_faiss_index(embeddings: List[List[float]]) -> faiss.IndexFlatIP:
    import numpy as np
    arr = np.array(embeddings, dtype="float32")
    faiss.normalize_L2(arr)
    index = faiss.IndexFlatIP(arr.shape[1])
    index.add(arr)
    return index


def search_faiss(index: faiss.IndexFlatIP, query_vec, top_k: int = TOP_K):
    import numpy as np
    q = np.array([query_vec], dtype="float32")
    faiss.normalize_L2(q)
    scores, idx = index.search(q, top_k)
    return scores[0].tolist(), idx[0].tolist()

# ----------------------------
# Reranking (BM25 simple)
# ----------------------------
class SimpleHybrid:
    def __init__(self, texts: List[str]):
        tokenized = [t.split() for t in texts]
        self.bm25 = BM25Okapi(tokenized)
        self.texts = texts
    def score(self, query: str, candidates: List[int], k: int = TOP_K) -> List[int]:
        # Re-rank the candidate indices by BM25 score w.r.t query
        doc_scores = self.bm25.get_scores(query.split())
        pairs = [(i, doc_scores[i]) for i in candidates]
        pairs.sort(key=lambda x: x[1], reverse=True)
        return [i for (i, _) in pairs[:k]]

# ----------------------------
# LLM Backends
# ----------------------------
LLM_SYSTEM_PROMPT = (
    "ë„ˆëŠ” â€˜ì¡°ê°íˆ¬ì ë„ë©”ì¸ ì–´ì‹œìŠ¤í„´íŠ¸â€™ì•¼. ì œê³µëœ CONTEXTë§Œì„ ê·¼ê±°ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´. "
    "í•­ìƒ [ì‚¬ìš©ì ì„±í–¥]ì˜ ë¦¬ìŠ¤í¬, ì˜ˆì‚°, ëª©í‘œë¥¼ ë°˜ë“œì‹œ ë¦¬í¬íŠ¸ì— ë°˜ì˜í•´ì•¼ í•œë‹¤. "
    "ìˆ«ì/ê¸°ê°„/ìˆ˜ìµë¥  ë“±ì€ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ â€˜ìë£Œ ë¶€ì¡±â€™ì´ë¼ê³  ë§í•´. "
    "íˆ¬ì ê¶Œìœ ì²˜ëŸ¼ ë‹¨ì •í•˜ì§€ ë§ê³  ì •ë³´ ì œê³µ ê´€ì ìœ¼ë¡œ ì‘ì„±í•´. "
    "ë‹µë³€ì€ ë°˜ë“œì‹œ ì•„ë˜ ë¦¬í¬íŠ¸ í…œí”Œë¦¿ êµ¬ì¡°ë¡œ ì‘ì„±í•œë‹¤:\n"
    "A. í•µì‹¬ìš”ì•½ (3ì¤„ ì´ë‚´, ì‚¬ìš©ì ì„±í–¥ 1ì¤„ ë°˜ì˜)\n"
    "B. ìˆ˜ìµÂ·ë¹„ìš© êµ¬ì¡° ìš”ì•½ (ê·¼ê±° ë¬¸ì¥ ëì— [ë²ˆí˜¸])\n"
    "C. ì˜ˆì‚°ë³„ ì „ëµ (ì‚¬ìš©ì ì˜ˆì‚° ê¸°ì¤€ ë¶„í• /í‹°ì¼“/ìœ ë™ì„±)\n"
    "D. ë¦¬ìŠ¤í¬ í¬ì¸íŠ¸ (ì‚¬ìš©ì ë¦¬ìŠ¤í¬ ì„ í˜¸ ë°˜ì˜)\n"
    "E. ëª©í‘œ ì í•©ì„± í‰ê°€ (ì‚¬ìš©ì ëª©í‘œ ê¸°ì¤€, [ë²ˆí˜¸])\n"
    "F. ìœ ì‚¬ìƒí’ˆ ë¹„êµ (ìˆìœ¼ë©´, [ë²ˆí˜¸])\n"
    "G. ì¶œì²˜ (ë²ˆí˜¸, ì œëª©, URL)\n"
    "H. ì•ˆì „ ë¬¸êµ¬ (â€˜ë³¸ ì„œë¹„ìŠ¤ëŠ” ì •ë³´ ì œê³µìš©ì´ë©°, ìˆ˜ìµ ë³´ì¥ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.â€™)\n"
)



def build_user_prompt(
    user_query: str,
    passages: List[Dict[str, Any]],
    risk: str,
    budget: int,
    horizon: str
) -> str:
    context_lines = []
    for i, p in enumerate(passages, start=1):
        title = p.get("title", "Untitled")
        url = p.get("url", "")
        date = p.get("as_of_date", "")
        text = p.get("text", "")
        context_lines.append(f"[{i}] {title} | {date} | {url}\n{text}")
    context_str = "\n\n".join(context_lines)

    user_prompt = (
        f"[ì‚¬ìš©ì ì„±í–¥]\në¦¬ìŠ¤í¬={risk}, ì˜ˆì‚°={budget}ì›, ëª©í‘œ={horizon}\n\n"
        f"[ì§ˆë¬¸]\n{user_query}\n\n"
        f"[CONTEXT]\n{context_str}\n"
    )
    return user_prompt



def call_ollama(model: str, system: str, prompt: str, host: str = OLLAMA_HOST) -> str:
    url = f"{host}/api/chat"
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system},
            {"role": "user", "content": prompt},
        ],
        "stream": False,
        "options": {
            "temperature": 0.2,
        },
    }
    r = requests.post(url, json=payload, timeout=120)
    r.raise_for_status()
    data = r.json()
    return data.get("message", {}).get("content", "")


def call_openai(model: str, system: str, prompt: str, api_key: str) -> str:
    import json as _json
    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system},
            {"role": "user", "content": prompt},
        ],
        "temperature": 0.2,
    }
    r = requests.post(url, headers=headers, data=_json.dumps(payload), timeout=120)
    r.raise_for_status()
    data = r.json()
    return data["choices"][0]["message"]["content"]

# ----------------------------
# Persistence helpers
# ----------------------------

def index_paths() -> Tuple[str, str]:
    faiss_path = os.path.join(INDEX_DIR, "faiss.index")
    meta_path = os.path.join(INDEX_DIR, "meta.json")
    return faiss_path, meta_path


def save_index(index: faiss.IndexFlatIP, meta: Dict[str, Any]):
    faiss_path, meta_path = index_paths()
    os.makedirs(os.path.dirname(faiss_path), exist_ok=True)
    try:
        faiss.write_index(index, faiss_path)
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)
    except Exception as e:
        # Windows + non-ASCII path bug: fallback to C:	upick_data
        if os.name == "nt":
            fallback_base = r"C:\tupick_data"
            fb_index_dir = os.path.join(fallback_base, "index")
            os.makedirs(fb_index_dir, exist_ok=True)
            fb_faiss = os.path.join(fb_index_dir, "faiss.index")
            fb_meta = os.path.join(fb_index_dir, "meta.json")
            faiss.write_index(index, fb_faiss)
            with open(fb_meta, "w", encoding="utf-8") as f:
                json.dump(meta, f, ensure_ascii=False, indent=2)
            try:
                import streamlit as _st
                _st.warning(f"ê¸°ë³¸ ê²½ë¡œì— ì“°ê¸° ì‹¤íŒ¨í•˜ì—¬ ASCII ê²½ë¡œë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {fb_faiss}")
            except Exception:
                pass
        else:
            raise


def load_index() -> Tuple[Any, Dict[str, Any]]:
    faiss_path, meta_path = index_paths()
    # primary
    if os.path.exists(faiss_path) and os.path.exists(meta_path):
        index = faiss.read_index(faiss_path)
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)
        return index, meta
    # fallback for Windows ASCII path
    if os.name == "nt":
        fb_index_dir = os.path.join(r"C:\tupick_data", "index")
        fb_faiss = os.path.join(fb_index_dir, "faiss.index")
        fb_meta = os.path.join(fb_index_dir, "meta.json")
        if os.path.exists(fb_faiss) and os.path.exists(fb_meta):
            index = faiss.read_index(fb_faiss)
            with open(fb_meta, "r", encoding="utf-8") as f:
                meta = json.load(f)
            return index, meta
    return None, {}


# ----------------------------
# Streamlit UI
# ----------------------------

st.set_page_config(page_title="Tupick â€” ì¡°ê°íˆ¬ì AI(MVP)", layout="wide")
st.title("Tupick â€” ì¡°ê°íˆ¬ì ì„œë¹„ìŠ¤ AI Â· RAG MVP")
st.caption("ë³¸ ì„œë¹„ìŠ¤ëŠ” ì •ë³´ ì œê³µìš© ë°ëª¨ì…ë‹ˆë‹¤. ìˆ˜ìµì„ ë³´ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

with st.sidebar:
    st.header("ë°ì´í„° ìˆ˜ì§‘ & ì¸ë±ì‹±")
    st.write("ìˆ˜ì§‘í•  URLì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”.")
    urls_text = st.text_area("URLs", height=200, placeholder="https://tessa.art/faq\nhttps://www.musicow.com/about/guide\nhttps://www.kasa.co.kr/faq")
    source_name = st.text_input("ì†ŒìŠ¤ ì‹ë³„ì(ì„œë¹„ìŠ¤ëª…)", value="generic")
    use_js = st.checkbox("JS ë Œë”ë§(Playwright) ì‚¬ìš©", value=True, help="ë®¤ì§ì¹´ìš°/ì¹´ì‚¬ì²˜ëŸ¼ SPA/ì°¨ë‹¨ëœ í˜ì´ì§€ ëŒ€ì‘")
    if st.button("ìˆ˜ì§‘â†’ì¸ë±ì‹± ì‹¤í–‰", type="primary"):
        urls = [u.strip() for u in urls_text.splitlines() if u.strip()]
        if not urls:
            st.error("URLì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        else:
            with st.spinner("ìˆ˜ì§‘ ë° ì¸ë±ì‹± ì¤‘..."):
                docs = []
                for u in urls:
                    try:
                        title, body = fetch_url(u, use_js=use_js)
                        cat = guess_category_from_url(u)
                        for ch in chunk_text(body):
                            did = f"{source_name}_{uuid.uuid4().hex[:12]}"
                            docs.append(
                                DocChunk(
                                    id=did,
                                    source=source_name,
                                    category=cat,
                                    title=title,
                                    section="ë³¸ë¬¸",
                                    url=u,
                                    as_of_date=now_date(),
                                    text=ch,
                                ).model_dump()
                            )
                    except Exception as e:
                        st.warning(f"ìˆ˜ì§‘ ì‹¤íŒ¨: {u} â€” {e}")
                if docs:
                    save_jsonl(docs, DOCS_PATH)
                    st.success(f"ìˆ˜ì§‘ ì™„ë£Œ: {len(docs)} ì²­í¬ ì €ì¥")

            # ì¸ë±ì‹±
            with st.spinner("ì„ë² ë”© ë° ì¸ë±ì‹± ì¤‘..."):
                all_docs = load_jsonl(DOCS_PATH)
                texts = [d["text"] for d in all_docs]
                embedder = get_embedder()
                vecs = embedder.encode(texts, normalize_embeddings=True).tolist()
                index = build_faiss_index(vecs)
                save_index(index, {"count": len(texts)})
                st.success(f"ì¸ë±ì‹± ì™„ë£Œ: {len(texts)} ê°œ ë¬¸ì„œ")

st.divider()

col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("ì§ˆì˜Â·ë¦¬í¬íŠ¸ ìƒì„±")
    user_query = st.text_area("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", height=120, placeholder="ì˜ˆ) í…Œì‚¬ ìˆ˜ìˆ˜ë£Œ êµ¬ì¡°ì™€ ë¶„ë°° ì£¼ê¸°ë¥¼ ìš”ì•½í•´ì¤˜. ì´ˆë³´ì ê¸°ì¤€ìœ¼ë¡œ ê°„ë‹¨íˆ, 100ë§Œì› ì˜ˆì‚° ì „ëµë„ í¬í•¨")

    persona_col1, persona_col2, persona_col3 = st.columns(3)
    with persona_col1:
        risk = st.selectbox("ë¦¬ìŠ¤í¬ ì„ í˜¸", ["ë‚®ìŒ", "ì¤‘ê°„", "ë†’ìŒ"], index=1)
    with persona_col2:
        budget = st.number_input("ì˜ˆì‚°(ì›)", min_value=0, value=1_000_000, step=100_000)
    with persona_col3:
        horizon = st.selectbox("ëª©í‘œ", ["ë‹¨ê¸°í˜„ê¸ˆíë¦„", "ì¤‘ê¸°ìˆ˜ìµ", "ì¥ê¸°ì„±ì¥"], index=0)

    if st.button("ê²€ìƒ‰â†’ìƒì„±", type="primary"):
        index, meta = load_index()
        if index is None:
            st.error("ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢Œì¸¡ì—ì„œ ë¨¼ì € ìˆ˜ì§‘â†’ì¸ë±ì‹±ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        else:
            all_docs = load_jsonl(DOCS_PATH)
            texts = [d["text"] for d in all_docs]

            embedder = get_embedder()
            qvec = embedder.encode([user_query], normalize_embeddings=True)[0]
            scores, idxs = search_faiss(index, qvec, top_k=max(TOP_K, 8))

            # Hybrid re-rank
            rer = SimpleHybrid(texts)
            re_idx = rer.score(user_query, idxs, k=TOP_K)

            top_passages = [all_docs[i] for i in re_idx]

            # Build prompt
            enriched_query = (
                f"[ì‚¬ìš©ì ì„±í–¥] ë¦¬ìŠ¤í¬={risk}, ì˜ˆì‚°={budget}ì›, ëª©í‘œ={horizon}\n"
                f"[ì§ˆë¬¸] {user_query}"
            )
            prompt = build_user_prompt(
                          user_query=user_query,
                          passages=top_passages,
                          risk=risk,
                          budget=budget,
                          horizon=horizon
                        )

            # Call LLM
            try:
                if OPENAI_API_KEY:
                    answer = call_openai(OPENAI_MODEL, LLM_SYSTEM_PROMPT, prompt, OPENAI_API_KEY)
                else:
                    answer = call_ollama(OLLAMA_MODEL, LLM_SYSTEM_PROMPT, prompt, OLLAMA_HOST)
            except Exception as e:
                st.error(f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                answer = ""

            if answer:
                st.markdown("### ë‹µë³€")
                st.write(answer)

                # Download report as Markdown
                md = f"# Tupick ë¦¬í¬íŠ¸\n\n**ì‘ì„±ì¼:** {now_date()}\n\n{answer}\n"
                b = md.encode("utf-8")
                st.download_button("ë¦¬í¬íŠ¸(MD) ë‹¤ìš´ë¡œë“œ", data=b, file_name="tupick_report.md", mime="text/markdown")

with col2:
    st.subheader("ì¸ë±ìŠ¤ ìƒíƒœ")
    index, meta = load_index()
    if index is not None:
        st.success(f"ì¸ë±ìŠ¤ ë¡œë“œë¨ â€” ë¬¸ì„œ ìˆ˜: {meta.get('count')}")
    else:
        st.info("ì•„ì§ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("#### ìœ ìš©í•œ íŒ")
    st.markdown(
    "- SPA(ìë°”ìŠ¤í¬ë¦½íŠ¸ ë Œë”) í˜ì´ì§€ëŠ” â€˜JS ë Œë”ë§â€™ ì²´í¬ë¥¼ ì¼œì„¸ìš”. "
    "ì²˜ìŒì—” `pip install playwright` í›„ `playwright install chromium` í•„ìš”.\n"
    "- ì§ˆì˜ì— ì„œë¹„ìŠ¤ëª…/í‚¤ì›Œë“œë¥¼ ë„£ìœ¼ë©´ ê²€ìƒ‰ ì •í™•ë„ê°€ ì˜¬ë¼ê°‘ë‹ˆë‹¤.\n"
    "- ê°™ì€ ì‚¬ì´íŠ¸ëŠ” í•œ ë²ˆì— 3~10ê°œ URL ì •ë„ë§Œ ë¨¼ì € ìˆ˜ì§‘í•´ë³´ì„¸ìš”.\n"
    "- ë¶„ë°°ì£¼ê¸°/ìˆ˜ìˆ˜ë£Œ/ë¦¬ìŠ¤í¬ ë“± ê·œì¹™ ë‹¨ì–´ë¥¼ í¬í•¨í•˜ë©´ ë¦¬í¬íŠ¸ í’ˆì§ˆì´ ì¢‹ì•„ì§‘ë‹ˆë‹¤."
)
st.divider()

st.caption(
    "â“˜ ì»´í”Œë¼ì´ì–¸ìŠ¤: ê° ì‚¬ì´íŠ¸ì˜ ì•½ê´€ ë° robots.txtë¥¼ ì¤€ìˆ˜í•´ì•¼ í•˜ë©°, ë³¸ ë°ëª¨ëŠ” êµìœ¡/ì—°êµ¬ìš©ì…ë‹ˆë‹¤."
)
